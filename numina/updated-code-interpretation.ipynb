{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.status.busy":"2024-04-19T17:52:56.027275Z","iopub.status.idle":"2024-04-19T17:52:56.027611Z","shell.execute_reply":"2024-04-19T17:52:56.027466Z","shell.execute_reply.started":"2024-04-19T17:52:56.027452Z"},"trusted":true},"outputs":[],"source":["## Forked From  https://kaggle.com/code/xiaoz259/pure-rng/notebook\n","\n","\n","# credits:\n","# https://www.kaggle.com/code/olyatsimboy/aimo-openmath-mistral-baseline\n","# https://www.kaggle.com/code/aatiffraz/prompt-prediction-w-mixtral-mistral7b-gemma-llama\n","# https://www.kaggle.com/code/thedrcat/aimo-mixtral-baseline"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T17:58:23.374735Z","iopub.status.busy":"2024-04-19T17:58:23.373709Z","iopub.status.idle":"2024-04-19T17:58:23.378812Z","shell.execute_reply":"2024-04-19T17:58:23.37786Z","shell.execute_reply.started":"2024-04-19T17:58:23.374701Z"},"trusted":true},"outputs":[],"source":["import time\n","\n","NOTEBOOK_START_TIME = time.time()"]},{"cell_type":"markdown","metadata":{},"source":["TO-DO\n","\n","Change temperature as the question goes longer\n","Change temperature based on question lenght"]},{"cell_type":"markdown","metadata":{},"source":["# Zero-shot MMOS-DeepSeekMath-7B with self-consistency and generated code reasoning evaluation\n","\n","Self-consistency is a modification of the standard greedy decoding in reasoning pipelines via sampling several diverse answers followed by aggregation, e.g., most common answer ([SC-CoT paper](https://arxiv.org/pdf/2203.11171.pdf)).\n","\n","In this kernel, we will consider MMOS-DeepSeekMath-7B RL-tuned backbone; in my experiments, this model produces more consistent code reasoning and the code block execution will allow us to decrease arithmetic hallucinations."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T17:53:39.559061Z","iopub.status.busy":"2024-04-19T17:53:39.558685Z","iopub.status.idle":"2024-04-19T17:53:39.564715Z","shell.execute_reply":"2024-04-19T17:53:39.563716Z","shell.execute_reply.started":"2024-04-19T17:53:39.559031Z"},"papermill":{"duration":18.075198,"end_time":"2024-02-29T09:25:25.295954","exception":false,"start_time":"2024-02-29T09:25:07.220756","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["DEBUG = False\n","\n","QUANT = False\n","\n","if QUANT:\n","    from transformers import BitsAndBytesConfig\n","    quantization_config = BitsAndBytesConfig(\n","        load_in_4bit = True,\n","        bnb_4bit_quant_type=\"nf4\",\n","        bnb_4bit_compute_dtype=torch.bfloat16,\n","        bnb_4bit_use_double_quant=True,\n","    )\n","\n","USE_PAST_KEY = True"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T17:53:39.818352Z","iopub.status.busy":"2024-04-19T17:53:39.817448Z","iopub.status.idle":"2024-04-19T17:53:58.835971Z","shell.execute_reply":"2024-04-19T17:53:58.835005Z","shell.execute_reply.started":"2024-04-19T17:53:39.818317Z"},"papermill":{"duration":18.075198,"end_time":"2024-02-29T09:25:25.295954","exception":false,"start_time":"2024-02-29T09:25:07.220756","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/mmu_nlp_hdd/suzhou03/models/aimo-progress-prize/myenv_python3.10_numina/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","/mmu_nlp_hdd/suzhou03/models/aimo-progress-prize/myenv_python3.10_numina/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Transformers Version: 4.43.0\n","CPU times: user 2.19 s, sys: 6.95 s, total: 9.14 s\n","Wall time: 1.49 s\n"]}],"source":["%%time\n","if QUANT:\n","    !pip install -U /kaggle/input/accelerate-wheelwhl/accelerate-0.29.1-py3-none-any.whl -qq\n","    !pip install -U /kaggle/input/bitsandbytes-0-42-0-py3-none-any-whl/bitsandbytes-0.42.0-py3-none-any.whl -qq\n","\n","\n","import torch\n","import gc\n","torch.backends.cuda.enable_mem_efficient_sdp(False)\n","\n","from transformers import (\n","    AutoModelForCausalLM, \n","    AutoTokenizer, \n","    AutoConfig,\n","    StoppingCriteria,\n","    set_seed\n",")\n","\n","import transformers\n","print(f\"Transformers Version: {transformers.__version__}\")\n","set_seed(42)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T17:53:58.838042Z","iopub.status.busy":"2024-04-19T17:53:58.837531Z","iopub.status.idle":"2024-04-19T17:53:58.862955Z","shell.execute_reply":"2024-04-19T17:53:58.862004Z","shell.execute_reply.started":"2024-04-19T17:53:58.838016Z"},"papermill":{"duration":1.224774,"end_time":"2024-02-29T09:36:31.21757","exception":false,"start_time":"2024-02-29T09:36:29.992796","status":"completed"},"tags":[],"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>row_id</th>\n","      <th>id</th>\n","      <th>problem</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>000aaa</td>\n","      <td>What is $1-1$?</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>111bbb</td>\n","      <td>What is $0\\times10$?</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>222ccc</td>\n","      <td>Solve $4+x=4$ for $x$.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   row_id      id                 problem\n","0       0  000aaa          What is $1-1$?\n","1       1  111bbb    What is $0\\times10$?\n","2       2  222ccc  Solve $4+x=4$ for $x$."]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","from tqdm import tqdm\n","PRIVATE = True\n","\n","df = pd.read_csv('/mmu_nlp_hdd/suzhou03/models/aimo-progress-prize/numina/ai-mathematical-olympiad-prize/test.csv')\n","df.head()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T17:53:58.864667Z","iopub.status.busy":"2024-04-19T17:53:58.86435Z","iopub.status.idle":"2024-04-19T17:53:58.880229Z","shell.execute_reply":"2024-04-19T17:53:58.87946Z","shell.execute_reply.started":"2024-04-19T17:53:58.864641Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>problem</th>\n","      <th>answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>229ee8</td>\n","      <td>Let $k, l &gt; 0$ be parameters. The parabola $y ...</td>\n","      <td>52</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>246d26</td>\n","      <td>Each of the three-digits numbers $111$ to $999...</td>\n","      <td>250</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2fc4ad</td>\n","      <td>Let the `sparkle' operation on positive intege...</td>\n","      <td>702</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>430b63</td>\n","      <td>What is the minimum value of $5x^2+5y^2-8xy$ w...</td>\n","      <td>800</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5277ed</td>\n","      <td>There exists a unique increasing geometric seq...</td>\n","      <td>211</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       id                                            problem  answer\n","0  229ee8  Let $k, l > 0$ be parameters. The parabola $y ...      52\n","1  246d26  Each of the three-digits numbers $111$ to $999...     250\n","2  2fc4ad  Let the `sparkle' operation on positive intege...     702\n","3  430b63  What is the minimum value of $5x^2+5y^2-8xy$ w...     800\n","4  5277ed  There exists a unique increasing geometric seq...     211"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["if len(df) < 5:\n","    df = pd.read_csv('/mmu_nlp_hdd/suzhou03/models/aimo-progress-prize/numina/ai-mathematical-olympiad-prize/train.csv')\n","    # PRIVATE = False\n","df.head()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T17:53:58.883409Z","iopub.status.busy":"2024-04-19T17:53:58.882824Z","iopub.status.idle":"2024-04-19T17:53:58.888319Z","shell.execute_reply":"2024-04-19T17:53:58.887477Z","shell.execute_reply.started":"2024-04-19T17:53:58.883385Z"},"trusted":true},"outputs":[],"source":["def naive_parse(answer):\n","    out = []\n","    start = False\n","    end = False\n","    for l in reversed(list(answer)):\n","        if l in '0123456789' and not end:\n","            start = True\n","            out.append(l)\n","        else:\n","            if start:\n","                end = True\n","        \n","    out = reversed(out)\n","    return ''.join(out)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T17:53:58.889893Z","iopub.status.busy":"2024-04-19T17:53:58.889639Z","iopub.status.idle":"2024-04-19T17:53:58.905267Z","shell.execute_reply":"2024-04-19T17:53:58.904389Z","shell.execute_reply.started":"2024-04-19T17:53:58.889872Z"},"trusted":true},"outputs":[],"source":["import re\n","import sys\n","import subprocess\n","\n","def return_last_print(output, n):\n","    lines = output.strip().split('\\n')\n","    if lines:\n","        return lines[n]\n","    else:\n","        return \"\"\n","\n","def process_code(code, return_shell_output=False):\n","    \n","    def repl(match):\n","        if \"real\" not in match.group():\n","            return \"{}{}\".format(match.group()[:-1], ', real=True)')\n","        else:\n","            return \"{}{}\".format(match.group()[:-1], ')')\n","    code = re.sub(r\"symbols\\([^)]+\\)\", repl, code)\n","\n","    if return_shell_output:\n","        code = code.replace('\\n', '\\n    ')\n","            # Add a try...except block\n","        code = \"\\ntry:\\n    from sympy import *\\n{}\\nexcept Exception as e:\\n    print(e)\\n    print('FAIL')\\n\".format(code)\n","    \n","    if not return_shell_output:\n","        print(code)\n","    with open('code.py', 'w') as fout:\n","        fout.write(code)\n","    \n","    batcmd = 'timeout 7 ' + sys.executable + ' code.py'\n","    try:\n","        shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n","        return_value = return_last_print(shell_output, -1)\n","        print(shell_output)\n","        if return_shell_output:\n","            if return_value=='FAIL':\n","                CODE_STATUS = False\n","                return_value = return_last_print(shell_output, -2)\n","                if \"not defined\" in return_value:\n","                    return_value+='\\nTry checking the formatting and imports'\n","            else:\n","                CODE_STATUS = True\n","            return return_value, CODE_STATUS  \n","        code_output = round(float(eval(return_value))) % 1000\n","    except Exception as e:\n","        print(e,'shell_output')\n","        code_output = -1\n","    \n","    if return_shell_output:\n","        if code_output==-1:\n","            CODE_STATUS = False\n","        else:\n","            CODE_STATUS = True\n","        return code_output, CODE_STATUS  \n","    \n","    \n","    return code_output\n","\n","\n","def process_text_output(output):\n","    result = output    \n","    try:\n","        result_output = re.findall(r'\\\\boxed\\{(\\d+)\\}', result)\n","\n","        print('BOXED', result_output)\n","        if not len(result_output):\n","            result_output = naive_parse(result)\n","        else:\n","            result_output = result_output[-1]\n","\n","        print('BOXED FINAL', result_output)\n","        if not len(result_output):\n","            result_output = -1\n","        \n","        else:\n","            result_output = round(float(eval(result_output))) % 1000\n","    \n","    except Exception as e:\n","        print(e)\n","        print('ERROR PARSING TEXT')\n","        result_output = -1\n","    \n","    return result_output\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T17:53:58.906601Z","iopub.status.busy":"2024-04-19T17:53:58.906315Z","iopub.status.idle":"2024-04-19T17:53:59.147724Z","shell.execute_reply":"2024-04-19T17:53:59.1468Z","shell.execute_reply.started":"2024-04-19T17:53:58.906579Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T17:53:59.149026Z","iopub.status.busy":"2024-04-19T17:53:59.148749Z","iopub.status.idle":"2024-04-19T17:53:59.158654Z","shell.execute_reply":"2024-04-19T17:53:59.157811Z","shell.execute_reply.started":"2024-04-19T17:53:59.149002Z"},"trusted":true},"outputs":[],"source":["import re\n","import math\n","import random\n","\n","from collections import defaultdict\n","\n","n_repetitions = 22 if PRIVATE else 4\n","TOTAL_TOKENS = 2048 # if PRIVATE else 512\n","\n","if PRIVATE:\n","    TIME_LIMIT = 31500\n","else:\n","    TIME_LIMIT = 1"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["True"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["PRIVATE"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T17:53:59.16021Z","iopub.status.busy":"2024-04-19T17:53:59.15988Z","iopub.status.idle":"2024-04-19T17:56:59.459049Z","shell.execute_reply":"2024-04-19T17:56:59.458113Z","shell.execute_reply.started":"2024-04-19T17:53:59.160167Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.01s/it]\n"]}],"source":["if PRIVATE:\n","\n","    MODEL_PATH = \"/mmu_nlp_hdd/suzhou03/models/aimo-progress-prize/data/numina-math-7b-tir\" #\"/nlp_group/decapoda-research/Llama-2-7b-chat-hf\"#\"/kaggle/input/gemma/transformers/7b-it/1\"\n","    DEEP = True\n","\n","    config = AutoConfig.from_pretrained(MODEL_PATH)\n","    config.gradient_checkpointing = True\n","\n","    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n","\n","    # device_map = [('model.embed_tokens', 0),\n","    #              ('model.layers.0', 0),\n","    #              ('model.layers.1', 0),\n","    #              ('model.layers.2', 0),\n","    #              ('model.layers.3', 0),\n","    #              ('model.layers.4', 0),\n","    #              ('model.layers.5', 0),\n","    #              ('model.layers.6', 0),\n","    #              ('model.layers.7', 0),\n","    #              ('model.layers.8', 0),\n","    #              ('model.layers.9', 0),\n","    #              ('model.layers.10', 0),\n","    #              ('model.layers.11', 0),\n","    #              ('model.layers.12', 0),\n","    #              ('model.layers.13', 0),\n","    #              ('model.layers.14', 0),\n","    #              ('model.layers.15', 0),\n","    #              ('model.layers.16', 0),\n","    #              ('model.layers.17', 0),\n","    #              ('model.layers.18', 0),\n","    #              ('model.layers.19', 0),\n","    #              ('model.layers.20', 0),\n","    #              ('model.layers.21', 0),\n","    #              ('model.layers.22', 1),\n","    #              ('model.layers.23', 1),\n","    #              ('model.layers.24', 1),\n","    #              ('model.layers.25', 1),\n","    #              ('model.layers.26', 1),\n","    #              ('model.layers.27', 1),\n","    #              ('model.layers.28', 1),\n","    #              ('model.layers.29', 1),\n","    #              ('model.norm', 1),\n","    #              ('lm_head', 1)]\n","\n","    # device_map = {ii:jj for (ii,jj) in device_map}\n","\n","    if QUANT:\n","        from transformers import BitsAndBytesConfig\n","        quantization_config = BitsAndBytesConfig(\n","            load_in_4bit = True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=torch.bfloat16,\n","            bnb_4bit_use_double_quant=True,\n","        )\n","        model = AutoModelForCausalLM.from_pretrained(\n","            MODEL_PATH,\n","            device_map=\"auto\",\n","            torch_dtype=\"auto\",\n","            trust_remote_code=True, \n","            quantization_config=quantization_config,\n","            config=config\n","        )\n","    else:  \n","        model = AutoModelForCausalLM.from_pretrained(\n","            MODEL_PATH,\n","            device_map=\"auto\",\n","            torch_dtype=\"auto\",\n","            trust_remote_code=True,\n","            #quantization_config=quantization_config,\n","            config=config\n","        )\n","    \n","    pipeline = transformers.pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    torch_dtype='auto',\n","    device_map=\"auto\",\n",")\n","    from transformers import StoppingCriteriaList\n","\n","    class StoppingCriteriaSub(StoppingCriteria):\n","        def __init__(self, stops = [], encounters=1):\n","            super().__init__()\n","            self.stops = [stop.to(\"cuda\") for stop in stops]\n","\n","        def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n","            for stop in self.stops:\n","                last_token = input_ids[0][-len(stop):]\n","                if torch.all(torch.eq(stop,last_token)):\n","                    return True\n","            return False\n","\n","\n","    stop_words = [\"```output\", \"```python\", \"```\\nOutput\" , \")\\n```\" , \"``````output\"] #,  \n","    stop_words_ids = [tokenizer(stop_word, return_tensors='pt', add_special_tokens=False)['input_ids'].squeeze() for stop_word in stop_words]\n","    stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])\n","    \n","    model.dtype, model.hf_device_map"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T18:30:10.316623Z","iopub.status.busy":"2024-04-19T18:30:10.315897Z","iopub.status.idle":"2024-04-19T18:30:10.322394Z","shell.execute_reply":"2024-04-19T18:30:10.321228Z","shell.execute_reply.started":"2024-04-19T18:30:10.316588Z"},"trusted":true},"outputs":[],"source":["code = \"\"\"Below is a math problem you are to solve (positive numerical answer):\n","\\\"{}\\\"\n","To accomplish this, first determine a sympy-based approach for solving the problem by listing each step to take and what functions need to be called in each step. Be clear so even an idiot can follow your instructions, and remember, your final answer should be positive integer, not an algebraic expression!\n","Write the entire script covering all the steps (use comments and document it well) and print the result. After solving the problem, output the final numerical answer within \\\\boxed{}.\n","\n","Approach:\"\"\"\n","\n","\n","cot = \"\"\"Below is a math problem you are to solve (positive numerical answer!):\n","\\\"{}\\\"\n","Analyze this problem and think step by step to come to a solution with programs. After solving the problem, output the final numerical answer within \\\\boxed{}.\\n\\n\"\"\"\n","\n","promplt_options = [code,cot]"]},{"cell_type":"code","execution_count":15,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-04-19T18:30:21.778859Z","iopub.status.busy":"2024-04-19T18:30:21.778508Z","iopub.status.idle":"2024-04-19T18:38:08.901206Z","shell.execute_reply":"2024-04-19T18:38:08.900084Z","shell.execute_reply.started":"2024-04-19T18:30:21.778834Z"},"papermill":{"duration":34.259365,"end_time":"2024-02-29T09:37:05.548829","exception":false,"start_time":"2024-02-29T09:36:31.289464","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/10 [00:00<?, ?it/s]\n","  0%|          | 0/22 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","QUESTION 0 - 0 - TIME_SPENT : 67 secs\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"ename":"NameError","evalue":"name 'best_stats' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/mmu_nlp_hdd/suzhou03/models/aimo-progress-prize/numina/updated-code-interpretation.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://kml-dtmachine-12755-prod.kml-hb2az1-l3-3-ingress.corp.kuaishou.com/mmu_nlp_hdd/suzhou03/models/aimo-progress-prize/numina/updated-code-interpretation.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m problem \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mproblem\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mloc[i]\n\u001b[1;32m     <a href='vscode-notebook-cell://kml-dtmachine-12755-prod.kml-hb2az1-l3-3-ingress.corp.kuaishou.com/mmu_nlp_hdd/suzhou03/models/aimo-progress-prize/numina/updated-code-interpretation.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mQUESTION \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m - \u001b[39m\u001b[39m{\u001b[39;00mjj\u001b[39m}\u001b[39;00m\u001b[39m - TIME_SPENT : \u001b[39m\u001b[39m{\u001b[39;00mTIME_SPENT\u001b[39m:\u001b[39;00m\u001b[39m.0f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m secs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://kml-dtmachine-12755-prod.kml-hb2az1-l3-3-ingress.corp.kuaishou.com/mmu_nlp_hdd/suzhou03/models/aimo-progress-prize/numina/updated-code-interpretation.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m best, best_count \u001b[39m=\u001b[39m best_stats\u001b[39m.\u001b[39mget(i, (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://kml-dtmachine-12755-prod.kml-hb2az1-l3-3-ingress.corp.kuaishou.com/mmu_nlp_hdd/suzhou03/models/aimo-progress-prize/numina/updated-code-interpretation.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mif\u001b[39;00m best_count \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39msqrt(jj):\n\u001b[1;32m     <a href='vscode-notebook-cell://kml-dtmachine-12755-prod.kml-hb2az1-l3-3-ingress.corp.kuaishou.com/mmu_nlp_hdd/suzhou03/models/aimo-progress-prize/numina/updated-code-interpretation.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSKIPPING CAUSE ALREADY FOUND BEST: \u001b[39m\u001b[39m{\u001b[39;00mbest_count\u001b[39m}\u001b[39;00m\u001b[39m, jj=\u001b[39m\u001b[39m{\u001b[39;00mjj\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'best_stats' is not defined"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["def process_code(code, return_shell_output=True):\n","    # Implementation of process_code function\n","    pass\n","\n","# Assuming this function is defined elsewhere\n","def process_text_output(text):\n","    # Implementation of process_text_output function\n","    pass\n","\n","for jj in tqdm(range(n_repetitions)):   \n","    for i in tqdm(range(len(df))):\n","        TIME_SPENT = time.time() - NOTEBOOK_START_TIME\n","        \n","        if TIME_SPENT > TIME_LIMIT:\n","            break\n","\n","        id_ = df['id'].loc[i]\n","        problem = df['problem'].loc[i]\n","        print(f\"\\n\\n\\nQUESTION {i} - {jj} - TIME_SPENT : {TIME_SPENT:.0f} secs\")\n","        \n","        best, best_count = best_stats.get(i, (-1, -1))\n","        if best_count > np.sqrt(jj):\n","            print(f\"SKIPPING CAUSE ALREADY FOUND BEST: {best_count}, jj={jj}\")\n","            continue\n","            \n","        outputs = total_outputs.get(i, [])\n","        text_answers, code_answers = question_type_counts.get(i, starting_counts)\n","        results = total_results.get(i, [])\n","        answers = total_answers.get(i, [])\n","        \n","        for _ in range(5):\n","            torch.cuda.empty_cache()\n","            gc.collect()\n","            time.sleep(0.2)\n","\n","        try:\n","            ALREADY_GEN = 0\n","            code_error = None\n","            code_error_count = 0\n","            code_output = None  # Initialize to None instead of -1\n","            counts = np.array([text_answers, code_answers])\n","\n","            draw = choice(promplt_options, 1, p=counts/counts.sum())\n","\n","            initail_message = draw[0].format(problem, \"{}\")            \n","            prompt = f\"User: {initail_message}\"\n","\n","            current_printed = len(prompt)\n","            print(f\"{jj}_{prompt}\\n\")\n","\n","            model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n","            input_len = len(model_inputs['input_ids'][0])\n","\n","            generation_output = model.generate(**model_inputs, \n","                                               max_new_tokens=TOTAL_TOKENS-ALREADY_GEN,\n","                                               return_dict_in_generate=USE_PAST_KEY,\n","                                               do_sample=True,\n","                                               temperature=temperature,\n","                                               top_p=top_p,\n","                                               num_return_sequences=1)\n","\n","            if USE_PAST_KEY:\n","                output_ids = generation_output.sequences[0]\n","            else:\n","                output_ids = generation_output[0]\n","            decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True)\n","            print(f\"{decoded_output[current_printed:]}\\n\")\n","            current_printed += len(decoded_output[current_printed:])\n","            cummulative_code = \"\"\n","            \n","            stop_word_cond = any(decoded_output.endswith(stop_word) for stop_word in stop_words)\n","            \n","            while (stop_word_cond) and (ALREADY_GEN < (TOTAL_TOKENS)):\n","                if (decoded_output[-len(\"```python\"):] == \"```python\"):\n","                    temperature_inner = temperature_coding\n","                    top_p_inner = top_p_coding\n","                    prompt = decoded_output\n","                else:\n","                    temperature_inner = temperature\n","                    top_p_inner = top_p\n","                    try:\n","                        if (decoded_output[-len(\"``````output\"):] == \"``````output\"):\n","                            code_text = decoded_output.split('```python')[-1].split(\"``````\")[0]\n","                        else:\n","                            code_text = decoded_output.split('```python')[-1].split(\"```\")[0]\n","                        \n","                        cummulative_code += code_text\n","                        print(\"Debug - Executing code:\", cummulative_code)\n","                        code_output, CODE_STATUS = process_code(cummulative_code, return_shell_output=True)\n","                        print('Debug - CODE RESULTS:', code_output)\n","                        print('Debug - CODE STATUS:', CODE_STATUS)\n","\n","                        if not CODE_STATUS:\n","                            print(\"Debug - Code execution failed\")\n","                            cummulative_code = cummulative_code[:-len(code_text)]\n","                            if code_error_count >= 1:\n","                                print(\"REPEATED ERRORS\")\n","                                break\n","                            code_error_count += 1\n","                        else:\n","                            code_error_count = 0\n","\n","                    except Exception as e:\n","                        print(f\"Error parsing or executing code: {e}\")\n","                        code_output = None\n","\n","                    if code_output is not None:\n","                        if (decoded_output[-len(\")\\n```\"):] == \")\\n```\"):\n","                            prompt = decoded_output + '```output\\n' + str(code_output) + '\\n```\\n'\n","                        else:\n","                            prompt = decoded_output + '\\n' + str(code_output) + '\\n```\\n'\n","                    else:\n","                        prompt = decoded_output\n","                        cummulative_code = \"\"\n","\n","                model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n","                ALREADY_GEN = len(model_inputs['input_ids'][0]) - input_len\n","\n","                if USE_PAST_KEY:\n","                    old_values = generation_output.past_key_values\n","                else:\n","                    old_values = None\n","\n","                generation_output = model.generate(**model_inputs, \n","                                                   max_new_tokens=TOTAL_TOKENS-ALREADY_GEN, \n","                                                   return_dict_in_generate=USE_PAST_KEY,\n","                                                   past_key_values=old_values,\n","                                                   do_sample=True,\n","                                                   temperature=temperature_inner,\n","                                                   top_p=top_p_inner,\n","                                                   num_return_sequences=1)\n","\n","                if USE_PAST_KEY:\n","                    output_ids = generation_output.sequences[0]\n","                else:\n","                    output_ids = generation_output[0]\n","                decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True)\n","                print(f\"\\nINTERMEDIATE OUT :\\n{decoded_output[current_printed:]}\\n\")\n","                current_printed += len(decoded_output[current_printed:])\n","                \n","                stop_word_cond = any(decoded_output.endswith(stop_word) for stop_word in stop_words)\n","\n","            if USE_PAST_KEY:\n","                output_ids = generation_output.sequences[0]\n","            else:\n","                output_ids = generation_output[0]\n","\n","            raw_output = tokenizer.decode(output_ids[input_len:], skip_special_tokens=True)\n","            result_output = process_text_output(raw_output)\n","            \n","            print(\"Debug - Final code_output before evaluation:\", code_output)\n","            \n","            try:\n","                if code_output is None:\n","                    raise ValueError(\"No valid code output\")\n","                elif isinstance(code_output, (int, float)):\n","                    result = int(code_output) % 1000\n","                elif isinstance(code_output, str) and code_output.strip():\n","                    result = int(eval(code_output, {\"__builtins__\": None}, {})) % 1000\n","                else:\n","                    raise ValueError(f\"Invalid code_output: {code_output}\")\n","                \n","                code_output = result\n","                print(\"Debug - Successful evaluation. Result:\", code_output)\n","            except Exception as e:\n","                print(f\"Error during evaluation: {e}\")\n","                code_output = None\n","\n","        except Exception as e:\n","            print(f\"Unexpected error: {e}\")\n","            result_output, code_output = None, None\n","\n","        if code_output is not None:\n","            outputs.append(code_output)\n","            code_answers += 1\n","\n","        if result_output is not None:\n","            outputs.append(result_output)\n","            text_answers += 1\n","\n","        if len(outputs) > 0:\n","            occurrences = Counter(outputs).most_common()\n","            print(occurrences)\n","            if occurrences[0][1] > best_count:\n","                print(\"GOOD ANSWER UPDATED!\")\n","                best = occurrences[0][0]\n","                best_count = occurrences[0][1]\n","            if occurrences[0][1] > 5:\n","                print(\"ANSWER FOUND!\")\n","                break\n","\n","        results.append(result_output)\n","        answers.append(code_output)\n","        \n","        best_stats[i] = (best, best_count) \n","        question_type_counts[i] = (text_answers, code_answers)\n","        total_outputs[i] = outputs\n","        \n","        total_results[i] = results\n","        total_answers[i] = answers\n","\n","        print(\"code_answers\", code_answers-starting_counts[1], \"text_answers\", text_answers-starting_counts[0])\n","        if DEBUG:\n","            break\n","\n","print(\"Processing complete.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:20:34.129793Z","iopub.status.idle":"2024-04-19T14:20:34.130264Z","shell.execute_reply":"2024-04-19T14:20:34.130031Z","shell.execute_reply.started":"2024-04-19T14:20:34.130012Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","if PRIVATE:\n","    for ii in range(len(df)):\n","        a = total_answers[ii]\n","        b = total_answers[ii]\n","        a = np.array(a)\n","        b = np.array(b)\n","        print(a,b)\n","        a[a < 0] = b[a < 0]\n","\n","        pred = Counter(a.tolist()).most_common(2)\n","        print(pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:20:34.131871Z","iopub.status.idle":"2024-04-19T14:20:34.132296Z","shell.execute_reply":"2024-04-19T14:20:34.132095Z","shell.execute_reply.started":"2024-04-19T14:20:34.132077Z"},"trusted":true},"outputs":[],"source":["if PRIVATE:\n","    df['answer'] = [best_stats[ii][0] for ii in range(len(df))]\n","else:\n","    df['answer'] = 2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:20:34.13377Z","iopub.status.idle":"2024-04-19T14:20:34.134076Z","shell.execute_reply":"2024-04-19T14:20:34.133938Z","shell.execute_reply.started":"2024-04-19T14:20:34.133925Z"},"papermill":{"duration":0.021128,"end_time":"2024-02-29T09:37:05.574782","exception":false,"start_time":"2024-02-29T09:37:05.553654","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["df[['id','answer']].to_csv(\"submission.csv\", header=True, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:20:34.137693Z","iopub.status.idle":"2024-04-19T14:20:34.138025Z","shell.execute_reply":"2024-04-19T14:20:34.137886Z","shell.execute_reply.started":"2024-04-19T14:20:34.137873Z"},"trusted":true},"outputs":[],"source":["if not PRIVATE:\n","    df = pd.read_csv('/mmu_nlp_hdd/suzhou03/models/numina/ai-mathematical-olympiad-prize/train.csv')\n","    if PRIVATE:\n","        df['model_answer'] = [best_stats[ii][0] for ii in range(len(df))]\n","        df['match'] = df.answer == df.model_answer\n","        print(f'{df.match.sum()} matches in {len(df)} examples')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:20:34.138857Z","iopub.status.idle":"2024-04-19T14:20:34.139177Z","shell.execute_reply":"2024-04-19T14:20:34.139028Z","shell.execute_reply.started":"2024-04-19T14:20:34.139014Z"},"trusted":true},"outputs":[],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-19T14:20:34.14083Z","iopub.status.idle":"2024-04-19T14:20:34.141156Z","shell.execute_reply":"2024-04-19T14:20:34.141004Z","shell.execute_reply.started":"2024-04-19T14:20:34.14099Z"},"trusted":true},"outputs":[],"source":["with open('code.py', 'w') as fout:\n","    fout.write(\"print('done')\")\n","\n","batcmd = 'timeout 7 ' + sys.executable + ' code.py'\n","try:\n","    shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n","    print(shell_output)\n","except:\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8365361,"sourceId":73231,"sourceType":"competition"},{"datasetId":4281572,"sourceId":7369493,"sourceType":"datasetVersion"},{"datasetId":4720595,"sourceId":8012825,"sourceType":"datasetVersion"},{"datasetId":4728129,"sourceId":8023365,"sourceType":"datasetVersion"},{"datasetId":4748944,"sourceId":8052555,"sourceType":"datasetVersion"},{"modelInstanceId":8332,"sourceId":11261,"sourceType":"modelInstanceVersion"},{"modelInstanceId":8318,"sourceId":11264,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"papermill":{"default_parameters":{},"duration":724.728315,"end_time":"2024-02-29T09:37:08.760349","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-29T09:25:04.032034","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"21267b653022419eb6fc3f47aa4db8ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_926e7ccdad6440be85c76931860b744c","placeholder":"​","style":"IPY_MODEL_feef8334edb24f6da22e8bb1d8d80c67","value":"Loading checkpoint shards: 100%"}},"2144e851698b4707ad1c7fc29fe21b03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3963993becfa487c9ff725f211915e67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7a725e1b0cc4ad78a62beab5f663065","placeholder":"​","style":"IPY_MODEL_fdb32baaed7145d8a8024b615ef242ca","value":" 19/19 [10:48&lt;00:00, 33.24s/it]"}},"5882b6e860be4a0db012a64fc0704a3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_21267b653022419eb6fc3f47aa4db8ed","IPY_MODEL_d91eb83d016a4381828192a98f798f9b","IPY_MODEL_3963993becfa487c9ff725f211915e67"],"layout":"IPY_MODEL_6a892a5561f742bb9db9f13859c18e90"}},"6a892a5561f742bb9db9f13859c18e90":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"926e7ccdad6440be85c76931860b744c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d91eb83d016a4381828192a98f798f9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2144e851698b4707ad1c7fc29fe21b03","max":19,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e0693b32889c42b18b9a3844e045d048","value":19}},"e0693b32889c42b18b9a3844e045d048":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f7a725e1b0cc4ad78a62beab5f663065":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdb32baaed7145d8a8024b615ef242ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"feef8334edb24f6da22e8bb1d8d80c67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}
