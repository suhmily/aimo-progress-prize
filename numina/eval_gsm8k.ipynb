{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from vllm import LLM, SamplingParams\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Constants\n",
    "N = 3  # Number of candidates per problem\n",
    "M = 2  # Depth of generation\n",
    "BATCH_SIZE = 4  # Batch size for vLLM\n",
    "TIME_LIMIT = 18 * 3600  # 18 hours limit\n",
    "TEST_SIZE = 1  # Number of GSM8K problems to test\n",
    "\n",
    "# Initialize vLLM\n",
    "MODEL_PATH = \"/nlp_group/decapoda-research/Llama-2-7b-chat-hf\"\n",
    "llm = LLM(model=MODEL_PATH, tensor_parallel_size=1)  # Using all 8 GPUs\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    max_tokens=1024,\n",
    "    stop=[\"```\", \"```output\", \"```python\", \"```\\nOutput\", \")\\n```\"]\n",
    ")\n",
    "\n",
    "def generate_prompt(problem):\n",
    "    return f\"\"\"Solve this math problem using Python:\n",
    "\n",
    "{problem}\n",
    "\n",
    "Provide your solution as a Python function named 'solve_problem' that takes no arguments and returns the answer. Store the final answer in a variable named 'answer'. Enclose your code in triple backticks with 'python' specified, like this:\n",
    "\n",
    "```python\n",
    "def solve_problem():\n",
    "    # Your code here\n",
    "    answer = # Final calculated answer\n",
    "    return answer\n",
    "```\n",
    "\n",
    "Do not include any explanations or additional text outside the code block.\n",
    "\"\"\"\n",
    "\n",
    "def extract_code(completion):\n",
    "    code_block = re.search(r'```python\\s*(.*?)\\s*```', completion, re.DOTALL)\n",
    "    if code_block:\n",
    "        return code_block.group(1).strip()\n",
    "    return None\n",
    "\n",
    "def execute_code(code):\n",
    "    try:\n",
    "        # Add a print statement to ensure the final answer is output\n",
    "        code += \"\\nprint(f'The final answer is {solve_problem()}')\"\n",
    "        \n",
    "        # Create a dictionary to store local variables\n",
    "        local_vars = {}\n",
    "        \n",
    "        # Execute the code in a restricted environment\n",
    "        exec(code, {'__builtins__': {'print': print, 'int': int, 'float': float}}, local_vars)\n",
    "        \n",
    "        # Extract the answer from the local variables\n",
    "        if 'answer' in local_vars:\n",
    "            return local_vars['answer']\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error executing code: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_batch(problems):\n",
    "    prompts = [generate_prompt(problem) for problem in problems]\n",
    "    all_candidates = []\n",
    "\n",
    "    for _ in range(M):  # M attempts for each problem\n",
    "        outputs = llm.generate(prompts, sampling_params)\n",
    "        completions = [output.outputs[0].text for output in outputs]\n",
    "        \n",
    "        candidates = []\n",
    "        for completion in completions:\n",
    "            code = extract_code(completion)\n",
    "            if code:\n",
    "                result = execute_code(code)\n",
    "                candidates.append(result)\n",
    "            else:\n",
    "                candidates.append(None)\n",
    "        \n",
    "        all_candidates.append(candidates)\n",
    "        \n",
    "        # Update prompts for problems that didn't get a valid result\n",
    "        prompts = [prompt + \"\\n\\nYour previous attempt was incorrect or incomplete. Please try again.\"\n",
    "                   for prompt, candidate in zip(prompts, candidates) if candidate is None]\n",
    "        \n",
    "        if not prompts:  # If all problems got valid results, break\n",
    "            break\n",
    "\n",
    "    # Transpose all_candidates to group by problem\n",
    "    return list(map(list, zip(*all_candidates)))\n",
    "\n",
    "def evaluate_gsm8k():\n",
    "    dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "    test_data = dataset[\"test\"].select(range(TEST_SIZE))\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(0, len(test_data), BATCH_SIZE):\n",
    "        if time.time() - start_time > TIME_LIMIT:\n",
    "            logging.info(\"Time limit reached. Stopping evaluation.\")\n",
    "            break\n",
    "\n",
    "        batch = test_data[i:i+BATCH_SIZE]\n",
    "        problems = [item['question'] for item in batch]\n",
    "        true_answers = [int(item['answer'].split()[-1]) for item in batch]\n",
    "\n",
    "        all_candidates = process_batch(problems)\n",
    "\n",
    "        for candidates, true_answer in zip(all_candidates, true_answers):\n",
    "            candidates = [c for c in candidates if c is not None]\n",
    "            if candidates:\n",
    "                predicted_answer = Counter(candidates).most_common(1)[0][0]\n",
    "                if predicted_answer == true_answer:\n",
    "                    correct += 1\n",
    "            total += 1\n",
    "\n",
    "        logging.info(f\"Processed {total}/{TEST_SIZE} problems. Current accuracy: {correct}/{total} = {correct/total:.2%}\")\n",
    "\n",
    "    final_accuracy = correct / total\n",
    "    logging.info(f\"Final Accuracy: {correct}/{total} = {final_accuracy:.2%}\")\n",
    "    return final_accuracy\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        accuracy = evaluate_gsm8k()\n",
    "        print(f\"Evaluation completed. Final accuracy: {accuracy:.2%}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred during evaluation: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_python3.8_envisions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
